---

Copyright 2013 Mentor Graphics Corp.  All Rights Reserved.

---


### xtUML Project Analysis Note
# Parallel Synchronous Messages are not supported by the Verifier Interface to
# External Code (VIEC)

1. Abstract
-----------
This note discusses a fix to prevent deadlock under certain circumstances when
both components are realized with user supplied java classes.

2. History
----------

3. Document References
----------------------
[1] ClearQuest DEI dts0100924661 - Possible deadlock running VIEC (renamed to
    Parallel Synchronous Message calls not supported by VIEC)
[2] docmsg/development/notes/dts0100924661/dts0100924661dnt.md
[3] svn://phoenix.azt.mentorg.com/arch1/products/tiger/doc_repository/trunk/
                                           Models/Documentation/dts0100924661
[4] ClearQuest DEI dts0100963423 - Raise runtime error if realized code attempts
    parallel synchronous calling in VIEC
[5] ClearQuest DEI dts0100963425 - Support parallel synchronous calling in
    Verifier Interface to External Code (VIEC)
[6] ClearQuest DEI dts0100963429 - Ignore port OAL when Component is realized.

4. Background
-------------
This issue was reported by a customer trying to communicate between two realized
components. The test model would sometimes run but at other times would hang.

5. Requirements
---------------
#5.1 Verifier should execute any mixture of realized and modeled components
without hanging the models.

6. Analysis
-----------
The problem was traced to a classic deadlock. After processing an incoming
interface operation, a realized component must signal the sending component to
indicate that it can proceed with its execution. This signaling is done by
calling Notify on the waiting component as the last act in the method,
VerifierInvocationHandler.executeRealizedCode(). 

A deadlock risk exists because in the current revision of this class (CVS
revision 1.6), the Notify call is made within a critical block synchronized to
the local Component Instance. In order to complete the Notify call, a lock must
be obtained on the remote component instance. If the sending component is itself
trying to send another message to the local component, it will already have
obtained a lock on itself and will be trying to obtain a lock on the local
component. Thus, each component thread locks itself first, then attempts to
obtain the lock on the other component. This is a classic deadlock situation.
Note that this situation can only arise with two realized components, since a
modeled component would not be attempting to send a message while waiting for a
response from a previous one. In other words, in this situation there must be
more than one thread running sending component behavior.

One solution is to move the Notify call outside of the critical section. Since
the sending component thread must be blocked in the thread that actually sent
the message, it appears to be safe to perform the Notify immediately outside of
the critical section.

The original customer model is not convenient for showing the failure, since it
fails leaving a socket open. To clear this problem, it is necessary to restart
the host environment. A replacement much smaller model was created [3] that shows
the essence of the issue without relying on sockets. This smaller tighter, model
revealed that the fix to move the Notify call is not robust. The fix opens a
window, during which a notification from another running component can be lost.

Reference [3] also contains some sequence diagrams that illustrate the normal
and deadlocking scenarios, together with a third diagram that shows the failure
window exposed by the small test model. The following sections provide additional
narrative to the diagrams. The section headings match the appropriate diagram title
under package dts0100924661 in [3].

6.1 Component locking normal operation
In normal operation, a running component obtains a lock on the running Component
Instance by entering a synchronized block. When it comes time to send a synchronous
message, it calls Body.StartStackframeforMessage(). This is shown on the diagram as
'Enqueue Message'. This call results in Notify being called on the target Component
instance, which requires a lock on the target component instance so that any queue
manipulation cannot collide with the target components own execution behavior. The
sending component then enters a wait state until the target component has serviced
the message. By waiting, the sending component gives up all its currently held locks.
When the sending component has finished servicing the message, it notifies the sending
component which then proceeds. The return notification also requires a lock, but this
also succeeds because the sender is in its wait state. Note that the above described
behavior is identical between realized and modeled components, which is why realized
components can co-exist successfully with modeled components.

6.2 Component locking failure mode
In this scenario, the sending component is realized and spawns an additional thread.
It is this additional thread that is key to creating the reported issue. The main
sending component thread enqueues a message as described in 6.1. The receiving
component then grabs its own lock and starts to process the message. At the same
time, the spawned thread begins to run and enqueues a message. It does so following
the procedure in 6.1, but before it can do so, it must obtain a lock on its own local
component instance. This succeeds because the local sending component is waiting on
the receiving component. The spawned thread then blocks waiting for the target
components lock. Now, the target component completes processing the message sent by
the sending component main thread. When it tries to notify the sending component, it
blocks because the spawned thread is holding the sending components lock. The
Receiving Component and spawned sending component threads are now embraced in a
classic deadlock. Note that in the case supplied by the customer, the spawned thread
is actually a socket listener, so it may not be obvious to the user that they have
created this thread contention. 

6.3 Component locking late notify
In this scenario, the proposed change has been made so that the release of the
receiving component's lock is done before the Notify is sent. This appears to remove
the deadlock, because now the spawned thread can complete its notification and release
the lock when the synchronized section is exited in the Receiving Component instance.
Now the Receiving Component can complete its protocol and notify the Sending Component.
All is not well, however, as there is now a window opened where a notification can be
delivered to the receiving component when it is not ready for it. A notification
arriving during this window is missed. The result is that the model stops executing.
Inspecting the execution state of such a stalled a model shows an enqueued message in
the receiving component.  However, the receiving component is in a wait state because
it has gone idle. Normally, a notify would take it out of this state, but the notify
arrived prematurely and was lost.

6.4 Recommendation
It is therefore recommended that the fix described in [2], section 7 *not* be
applied. An issue [6] is raised for the cascade problems identified in [2], section
8. The headline issue [1] shall be closed with resolution 'Won't Fix.'. The
customer must be informed to look for alternative ways to achieve the goal. In
particular, the use of parallel synchronous messages is at the root of this problem,
because asynchronous messages do not require the sending component to block waiting
for the service of the message in the receiver.

As part of this analysis, it was considered whether a synchronous void message is not
the same as an asynchronous message (i.e. a signal). The consensus was that they are
semantically distinct.

The use of parallel synchronous calling is not fundamentally unsupported by the xtUML
method. But it is felt that this is too complex a study to complete as part of the
current cycle. Two additional issues are therefore raised; [4] covers the work to
detect and report parallel synchronous calls in the current VIEC design, and [5]
tracks a study to see if parallel synchronous calls can be supported robustly in the
future.

7. Work Required
----------------
None.

8. Acceptance Test
------------
None.

End
---
